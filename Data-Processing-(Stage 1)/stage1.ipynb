{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert\" style=\"background-color:#fff; color:white; padding:0px 10px; border-radius:5px;\"><h1 style='margin:15px 15px; color:#5d3a8e; font-size:40px'>Community Detection</h1>\n",
    "</div>\n",
    "\n",
    "***\n",
    "## Stage 1:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'> 1. Import Packages</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Rama\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#files packages\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pathlib\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "#stemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "\n",
    "#stopWords\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words(\"english\")\n",
    "\n",
    "# list of the names of the channels\n",
    "channels = ['techchap', \"dave2d\", \"ijustine\", \"mkbhd\", \"unboxtherapy\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'> 2. Tokenize words, remove Stopwords, Lemmatize, and Clean-up text </h2>\n",
    "</div>\n",
    "\n",
    "### Here we created a dataframe that contains the stems of each video subtitles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Youtube-Data/dave2d/dave2d/dave2d-subtitles\\RARKNr6ru-c.txt\n",
      "Youtube-Data/dave2d/dave2d/dave2d-subtitles\\sMib1nMCdfc.txt\n",
      "Youtube-Data/ijustine/ijustine/ijustine-subtitles\\B5a_k25lSAE.txt\n",
      "Youtube-Data/unboxtherapy/unboxtherapy/unboxtherapy-subtitles\\0JzXL_uAHfo.txt\n",
      "Youtube-Data/unboxtherapy/unboxtherapy/unboxtherapy-subtitles\\F1ZT8XyxyH4.txt\n",
      "Youtube-Data/unboxtherapy/unboxtherapy/unboxtherapy-subtitles\\G2ZE5W97kXE.txt\n",
      "Youtube-Data/unboxtherapy/unboxtherapy/unboxtherapy-subtitles\\kdQM2dUHuDc.txt\n",
      "Youtube-Data/unboxtherapy/unboxtherapy/unboxtherapy-subtitles\\l87M93p7PC0.txt\n",
      "Youtube-Data/unboxtherapy/unboxtherapy/unboxtherapy-subtitles\\T1wkbDxCa6o.txt\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "#looping over the channels\n",
    "for channel in channels:\n",
    "    #opening the directory of that channel\n",
    "    files = [f for f in listdir(\"Youtube-Data/\"+ channel + \"/\"+ channel + \"/\" + channel + \"-subtitles\")]\n",
    "    #looping over the files of channel\n",
    "    for file in files: \n",
    "        try:\n",
    "            with open (join(\"Youtube-Data/\"+ channel + \"/\"+ channel + \"/\" + channel + \"-subtitles\", file), \"r\") as myfile:\n",
    "                data=myfile.read().replace('\\n', ' ')\n",
    "            # clean the data from all punctuation \n",
    "            data = data.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "            nltk_tokens = nltk.word_tokenize(data)\n",
    "            cleanTokens = [x for x in nltk_tokens if not x in sw]\n",
    "\n",
    "            stems = []\n",
    "            ps = PorterStemmer()\n",
    "            for w in cleanTokens:\n",
    "                stems.append(ps.stem(WordNetLemmatizer().lemmatize(w, pos='v'))) \n",
    "\n",
    "\n",
    "            tempData = pd.DataFrame([[file, stems]])\n",
    "\n",
    "            df = df.append(tempData, ignore_index=True)\n",
    "        except:\n",
    "            print(join(\"Youtube-Data/\"+ channel + \"/\"+ channel + \"/\" + channel + \"-subtitles\", file))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for preprocessing texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    data = data.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    nltk_tokens = nltk.word_tokenize(data)\n",
    "    cleanTokens = [x for x in nltk_tokens if not x in sw]\n",
    "\n",
    "    stems = []\n",
    "    ps = PorterStemmer()\n",
    "    for w in cleanTokens:\n",
    "        stems.append(ps.stem(WordNetLemmatizer().lemmatize(w, pos='v'))) \n",
    "        \n",
    "    return stems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>stems</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-7gyHZEving.txt</td>\n",
       "      <td>[hey, guy, anton, tech, chap, new, msi, GF, 65...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-cOYX11AfPc.txt</td>\n",
       "      <td>[hey, guy, Im, tom, tech, chap, someth, littl,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-g1mHQwkpQY.txt</td>\n",
       "      <td>[hi, guy, welcom, back, tech, chap, peopl, thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-lCQMFC2D5Q.txt</td>\n",
       "      <td>[oh, that, way, heavier, I, expect, actual, Im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-My0ls6Da-c.txt</td>\n",
       "      <td>[hey, guy, im, tummi, tech, chapman, ive, get,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6483</th>\n",
       "      <td>_RU8FktAnlU.txt</td>\n",
       "      <td>[welcom, back, favorit, seri, unbox, therapi, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6484</th>\n",
       "      <td>_T3uDK90PvA.txt</td>\n",
       "      <td>[chair, mean, din, read, newspap, still, other...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6485</th>\n",
       "      <td>_uLIiWSqAAg.txt</td>\n",
       "      <td>[come, check, So, guy, follow, instagram, youv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6486</th>\n",
       "      <td>_VSC4iGYGQA.txt</td>\n",
       "      <td>[music, what, guy, lew, see, behind, final, ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6487</th>\n",
       "      <td>__RbDWQ2-ho.txt</td>\n",
       "      <td>[critic, listen, would, go, willi, nice, know,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6488 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 file                                              stems\n",
       "0     -7gyHZEving.txt  [hey, guy, anton, tech, chap, new, msi, GF, 65...\n",
       "1     -cOYX11AfPc.txt  [hey, guy, Im, tom, tech, chap, someth, littl,...\n",
       "2     -g1mHQwkpQY.txt  [hi, guy, welcom, back, tech, chap, peopl, thi...\n",
       "3     -lCQMFC2D5Q.txt  [oh, that, way, heavier, I, expect, actual, Im...\n",
       "4     -My0ls6Da-c.txt  [hey, guy, im, tummi, tech, chapman, ive, get,...\n",
       "...               ...                                                ...\n",
       "6483  _RU8FktAnlU.txt  [welcom, back, favorit, seri, unbox, therapi, ...\n",
       "6484  _T3uDK90PvA.txt  [chair, mean, din, read, newspap, still, other...\n",
       "6485  _uLIiWSqAAg.txt  [come, check, So, guy, follow, instagram, youv...\n",
       "6486  _VSC4iGYGQA.txt  [music, what, guy, lew, see, behind, final, ge...\n",
       "6487  __RbDWQ2-ho.txt  [critic, listen, would, go, willi, nice, know,...\n",
       "\n",
       "[6488 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns = [\"file\", \"stems\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'> 3. Create the Dictionary and Corpus needed for Topic Modeling</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "processed_docs = df['stems']\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'> 4. Building the Topic Model</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=30, id2word=dictionary, passes=2, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'> 5. View the topics in LDA model</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.005*\"phone\" + 0.003*\"camera\" + 0.003*\"iphon\" + 0.002*\"galaxi\" + 0.002*\"ipad\" + 0.002*\"android\" + 0.002*\"display\" + 0.002*\"screen\" + 0.002*\"pixel\" + 0.002*\"nexu\"\n",
      "Topic: 1 Word: 0.004*\"So\" + 0.003*\"thi\" + 0.002*\"and\" + 0.002*\"it\" + 0.002*\"iphon\" + 0.002*\"Oh\" + 0.002*\"laptop\" + 0.002*\"music\" + 0.002*\"the\" + 0.002*\"you\"\n",
      "Topic: 2 Word: 0.003*\"So\" + 0.002*\"googl\" + 0.002*\"oh\" + 0.002*\"game\" + 0.002*\"thi\" + 0.002*\"Oh\" + 0.002*\"phone\" + 0.002*\"emoji\" + 0.002*\"he\" + 0.002*\"app\"\n",
      "Topic: 3 Word: 0.003*\"So\" + 0.002*\"phone\" + 0.002*\"car\" + 0.002*\"it\" + 0.002*\"question\" + 0.002*\"thi\" + 0.002*\"iphon\" + 0.002*\"danc\" + 0.002*\"Oh\" + 0.002*\"and\"\n",
      "Topic: 4 Word: 0.005*\"So\" + 0.004*\"thi\" + 0.003*\"and\" + 0.003*\"Oh\" + 0.003*\"it\" + 0.002*\"phone\" + 0.002*\"you\" + 0.002*\"camera\" + 0.002*\"iphon\" + 0.002*\"the\"\n",
      "Topic: 5 Word: 0.003*\"laptop\" + 0.003*\"phone\" + 0.002*\"game\" + 0.002*\"camera\" + 0.002*\"So\" + 0.002*\"batteri\" + 0.002*\"pixel\" + 0.002*\"screen\" + 0.001*\"devic\" + 0.001*\"appl\"\n",
      "Topic: 6 Word: 0.003*\"iphon\" + 0.002*\"phone\" + 0.002*\"devic\" + 0.002*\"laptop\" + 0.002*\"app\" + 0.002*\"batteri\" + 0.002*\"headphon\" + 0.002*\"So\" + 0.002*\"game\" + 0.001*\"cabl\"\n",
      "Topic: 7 Word: 0.003*\"phone\" + 0.003*\"So\" + 0.003*\"iphon\" + 0.003*\"pro\" + 0.002*\"camera\" + 0.002*\"and\" + 0.002*\"macbook\" + 0.002*\"game\" + 0.002*\"laptop\" + 0.002*\"headphon\"\n",
      "Topic: 8 Word: 0.002*\"So\" + 0.002*\"phone\" + 0.002*\"oh\" + 0.002*\"thi\" + 0.002*\"devic\" + 0.002*\"game\" + 0.002*\"pro\" + 0.002*\"Oh\" + 0.002*\"camera\" + 0.002*\"iphon\"\n",
      "Topic: 9 Word: 0.004*\"phone\" + 0.004*\"iphon\" + 0.003*\"camera\" + 0.003*\"note\" + 0.002*\"s9\" + 0.002*\"plu\" + 0.002*\"screen\" + 0.002*\"appl\" + 0.002*\"batteri\" + 0.002*\"10\"\n",
      "Topic: 10 Word: 0.003*\"phone\" + 0.003*\"So\" + 0.002*\"thi\" + 0.002*\"Oh\" + 0.002*\"it\" + 0.002*\"justin\" + 0.002*\"pixel\" + 0.002*\"camera\" + 0.002*\"googl\" + 0.002*\"and\"\n",
      "Topic: 11 Word: 0.003*\"phone\" + 0.003*\"appl\" + 0.003*\"iphon\" + 0.002*\"pro\" + 0.002*\"laptop\" + 0.002*\"keyboard\" + 0.002*\"ipad\" + 0.002*\"game\" + 0.002*\"So\" + 0.002*\"app\"\n",
      "Topic: 12 Word: 0.004*\"So\" + 0.003*\"and\" + 0.003*\"phone\" + 0.002*\"thi\" + 0.002*\"headphon\" + 0.002*\"it\" + 0.002*\"game\" + 0.002*\"unbox\" + 0.002*\"Oh\" + 0.002*\"iphon\"\n",
      "Topic: 13 Word: 0.003*\"phone\" + 0.002*\"camera\" + 0.002*\"So\" + 0.002*\"iphon\" + 0.002*\"laptop\" + 0.002*\"it\" + 0.002*\"devic\" + 0.002*\"appl\" + 0.002*\"batteri\" + 0.002*\"game\"\n",
      "Topic: 14 Word: 0.004*\"phone\" + 0.003*\"camera\" + 0.003*\"iphon\" + 0.002*\"screen\" + 0.002*\"fold\" + 0.002*\"display\" + 0.002*\"devic\" + 0.002*\"appl\" + 0.002*\"pixel\" + 0.002*\"android\"\n",
      "Topic: 15 Word: 0.003*\"phone\" + 0.003*\"oh\" + 0.002*\"iphon\" + 0.002*\"So\" + 0.002*\"display\" + 0.002*\"game\" + 0.002*\"galaxi\" + 0.002*\"laptop\" + 0.001*\"camera\" + 0.001*\"music\"\n",
      "Topic: 16 Word: 0.002*\"safari\" + 0.002*\"phone\" + 0.002*\"So\" + 0.002*\"thi\" + 0.002*\"camera\" + 0.002*\"game\" + 0.002*\"cake\" + 0.002*\"oh\" + 0.002*\"Oh\" + 0.002*\"iphon\"\n",
      "Topic: 17 Word: 0.002*\"phone\" + 0.002*\"game\" + 0.002*\"So\" + 0.002*\"keyboard\" + 0.002*\"devic\" + 0.002*\"camera\" + 0.002*\"thi\" + 0.002*\"display\" + 0.002*\"it\" + 0.002*\"laptop\"\n",
      "Topic: 18 Word: 0.002*\"nexu\" + 0.002*\"laptop\" + 0.002*\"So\" + 0.002*\"googl\" + 0.002*\"phone\" + 0.002*\"youtub\" + 0.002*\"camera\" + 0.002*\"ipad\" + 0.002*\"and\" + 0.002*\"oh\"\n",
      "Topic: 19 Word: 0.003*\"phone\" + 0.002*\"iphon\" + 0.002*\"game\" + 0.002*\"camera\" + 0.002*\"galaxi\" + 0.002*\"app\" + 0.002*\"4k\" + 0.002*\"devic\" + 0.002*\"appl\" + 0.002*\"monitor\"\n",
      "Topic: 20 Word: 0.003*\"phone\" + 0.002*\"game\" + 0.002*\"oh\" + 0.002*\"laptop\" + 0.002*\"nexu\" + 0.002*\"camera\" + 0.002*\"googl\" + 0.002*\"pro\" + 0.002*\"iphon\" + 0.002*\"So\"\n",
      "Topic: 21 Word: 0.003*\"it\" + 0.003*\"So\" + 0.003*\"phone\" + 0.002*\"thi\" + 0.002*\"Oh\" + 0.002*\"oh\" + 0.002*\"game\" + 0.002*\"keyboard\" + 0.002*\"iphon\" + 0.002*\"cake\"\n",
      "Topic: 22 Word: 0.003*\"So\" + 0.003*\"thi\" + 0.003*\"phone\" + 0.003*\"it\" + 0.002*\"and\" + 0.002*\"Oh\" + 0.002*\"camera\" + 0.002*\"car\" + 0.002*\"you\" + 0.002*\"jenna\"\n",
      "Topic: 23 Word: 0.003*\"macbook\" + 0.003*\"So\" + 0.003*\"pro\" + 0.002*\"laptop\" + 0.002*\"phone\" + 0.002*\"appl\" + 0.002*\"thi\" + 0.002*\"devic\" + 0.002*\"it\" + 0.002*\"game\"\n",
      "Topic: 24 Word: 0.005*\"So\" + 0.004*\"laptop\" + 0.004*\"it\" + 0.004*\"thi\" + 0.003*\"the\" + 0.003*\"keyboard\" + 0.003*\"game\" + 0.003*\"but\" + 0.003*\"Oh\" + 0.003*\"and\"\n",
      "Topic: 25 Word: 0.003*\"pro\" + 0.002*\"phone\" + 0.002*\"laptop\" + 0.002*\"camera\" + 0.002*\"So\" + 0.002*\"headphon\" + 0.002*\"case\" + 0.002*\"appl\" + 0.002*\"macbook\" + 0.002*\"game\"\n",
      "Topic: 26 Word: 0.004*\"laptop\" + 0.003*\"game\" + 0.003*\"phone\" + 0.002*\"pro\" + 0.002*\"iphon\" + 0.002*\"devic\" + 0.002*\"camera\" + 0.002*\"macbook\" + 0.002*\"im\" + 0.002*\"screen\"\n",
      "Topic: 27 Word: 0.003*\"phone\" + 0.002*\"Ã¢â„¢Âª\" + 0.002*\"So\" + 0.002*\"laptop\" + 0.002*\"devic\" + 0.002*\"camera\" + 0.002*\"keyboard\" + 0.002*\"headphon\" + 0.002*\"galaxi\" + 0.002*\"thi\"\n",
      "Topic: 28 Word: 0.003*\"iphon\" + 0.003*\"laptop\" + 0.003*\"phone\" + 0.003*\"appl\" + 0.003*\"devic\" + 0.002*\"camera\" + 0.002*\"ipad\" + 0.002*\"pro\" + 0.002*\"display\" + 0.002*\"game\"\n",
      "Topic: 29 Word: 0.006*\"So\" + 0.005*\"thi\" + 0.004*\"it\" + 0.004*\"Oh\" + 0.004*\"and\" + 0.003*\"you\" + 0.003*\"jenna\" + 0.003*\"Ã¢â„¢Âª\" + 0.003*\"justin\" + 0.002*\"phone\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the topic of a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.7007796764373779\t Topic: 0.004*\"phone\" + 0.003*\"camera\" + 0.003*\"iphon\" + 0.002*\"screen\" + 0.002*\"fold\"\n",
      "Score: 0.19530367851257324\t Topic: 0.004*\"So\" + 0.003*\"and\" + 0.003*\"phone\" + 0.002*\"thi\" + 0.002*\"headphon\"\n",
      "Score: 0.0037113104481250048\t Topic: 0.005*\"phone\" + 0.003*\"camera\" + 0.003*\"iphon\" + 0.002*\"galaxi\" + 0.002*\"ipad\"\n",
      "Score: 0.0037113104481250048\t Topic: 0.004*\"So\" + 0.003*\"thi\" + 0.002*\"and\" + 0.002*\"it\" + 0.002*\"iphon\"\n",
      "Score: 0.0037113104481250048\t Topic: 0.003*\"So\" + 0.002*\"googl\" + 0.002*\"oh\" + 0.002*\"game\" + 0.002*\"thi\"\n",
      "Score: 0.0037113104481250048\t Topic: 0.003*\"So\" + 0.002*\"phone\" + 0.002*\"car\" + 0.002*\"it\" + 0.002*\"question\"\n",
      "Score: 0.0037113104481250048\t Topic: 0.005*\"So\" + 0.004*\"thi\" + 0.003*\"and\" + 0.003*\"Oh\" + 0.003*\"it\"\n",
      "Score: 0.0037113104481250048\t Topic: 0.003*\"laptop\" + 0.003*\"phone\" + 0.002*\"game\" + 0.002*\"camera\" + 0.002*\"So\"\n",
      "Score: 0.0037113104481250048\t Topic: 0.003*\"iphon\" + 0.002*\"phone\" + 0.002*\"devic\" + 0.002*\"laptop\" + 0.002*\"app\"\n",
      "Score: 0.0037113104481250048\t Topic: 0.003*\"phone\" + 0.003*\"So\" + 0.003*\"iphon\" + 0.003*\"pro\" + 0.002*\"camera\"\n",
      "Score: 0.0037113104481250048\t Topic: 0.002*\"So\" + 0.002*\"phone\" + 0.002*\"oh\" + 0.002*\"thi\" + 0.002*\"devic\"\n",
      "Score: 0.0037113104481250048\t Topic: 0.004*\"phone\" + 0.004*\"iphon\" + 0.003*\"camera\" + 0.003*\"note\" + 0.002*\"s9\"\n",
      "Score: 0.0037113104481250048\t Topic: 0.003*\"phone\" + 0.003*\"So\" + 0.002*\"thi\" + 0.002*\"Oh\" + 0.002*\"it\"\n",
      "Score: 0.0037113104481250048\t Topic: 0.003*\"phone\" + 0.003*\"appl\" + 0.003*\"iphon\" + 0.002*\"pro\" + 0.002*\"laptop\"\n",
      "Score: 0.0037113104481250048\t Topic: 0.003*\"phone\" + 0.002*\"camera\" + 0.002*\"So\" + 0.002*\"iphon\" + 0.002*\"laptop\"\n",
      "Score: 0.0037113104481250048\t Topic: 0.003*\"phone\" + 0.003*\"oh\" + 0.002*\"iphon\" + 0.002*\"So\" + 0.002*\"display\"\n",
      "Score: 0.0037113104481250048\t Topic: 0.002*\"safari\" + 0.002*\"phone\" + 0.002*\"So\" + 0.002*\"thi\" + 0.002*\"camera\"\n",
      "Score: 0.0037113104481250048\t Topic: 0.002*\"phone\" + 0.002*\"game\" + 0.002*\"So\" + 0.002*\"keyboard\" + 0.002*\"devic\"\n",
      "Score: 0.0037113104481250048\t Topic: 0.002*\"nexu\" + 0.002*\"laptop\" + 0.002*\"So\" + 0.002*\"googl\" + 0.002*\"phone\"\n",
      "Score: 0.0037113104481250048\t Topic: 0.003*\"phone\" + 0.002*\"iphon\" + 0.002*\"game\" + 0.002*\"camera\" + 0.002*\"galaxi\"\n",
      "Score: 0.0037113104481250048\t Topic: 0.003*\"phone\" + 0.002*\"game\" + 0.002*\"oh\" + 0.002*\"laptop\" + 0.002*\"nexu\"\n",
      "Score: 0.0037113104481250048\t Topic: 0.003*\"it\" + 0.003*\"So\" + 0.003*\"phone\" + 0.002*\"thi\" + 0.002*\"Oh\"\n",
      "Score: 0.0037113104481250048\t Topic: 0.003*\"So\" + 0.003*\"thi\" + 0.003*\"phone\" + 0.003*\"it\" + 0.002*\"and\"\n",
      "Score: 0.0037113104481250048\t Topic: 0.003*\"macbook\" + 0.003*\"So\" + 0.003*\"pro\" + 0.002*\"laptop\" + 0.002*\"phone\"\n",
      "Score: 0.0037113104481250048\t Topic: 0.005*\"So\" + 0.004*\"laptop\" + 0.004*\"it\" + 0.004*\"thi\" + 0.003*\"the\"\n",
      "Score: 0.0037113104481250048\t Topic: 0.003*\"pro\" + 0.002*\"phone\" + 0.002*\"laptop\" + 0.002*\"camera\" + 0.002*\"So\"\n",
      "Score: 0.0037113104481250048\t Topic: 0.004*\"laptop\" + 0.003*\"game\" + 0.003*\"phone\" + 0.002*\"pro\" + 0.002*\"iphon\"\n",
      "Score: 0.0037113104481250048\t Topic: 0.003*\"phone\" + 0.002*\"Ã¢â„¢Âª\" + 0.002*\"So\" + 0.002*\"laptop\" + 0.002*\"devic\"\n",
      "Score: 0.0037113104481250048\t Topic: 0.003*\"iphon\" + 0.003*\"laptop\" + 0.003*\"phone\" + 0.003*\"appl\" + 0.003*\"devic\"\n",
      "Score: 0.0037113104481250048\t Topic: 0.006*\"So\" + 0.005*\"thi\" + 0.004*\"it\" + 0.004*\"Oh\" + 0.004*\"and\"\n"
     ]
    }
   ],
   "source": [
    "unseen_document = '''The phones are great but I think they should have made the prices lower, to compete with apple.( coming from a Samsung fanboy)'''\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "for index, score in sorted(lda_model_tfidf[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model_tfidf.print_topic(index, 5)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "\n",
    "# Save model to disk.\n",
    "temp_file = datapath(\"C:/Users/Rama/nlp-datamining/communityDetection/LDAmodel\")\n",
    "lda_model_tfidf.save(temp_file)\n",
    "\n",
    "#save dictionary to disk\n",
    "dictionary.save(datapath(\"C:/Users/Rama/nlp-datamining/communityDetection/dictionary\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" style=\"background-color:#5d3a8e; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'> 6. Finding the dominant topic for each video subtitles</h2>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>stems</th>\n",
       "      <th>topicID</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-7gyHZEving</td>\n",
       "      <td>[hey, guy, anton, tech, chap, new, msi, GF, 65...</td>\n",
       "      <td>26</td>\n",
       "      <td>0.535545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-cOYX11AfPc</td>\n",
       "      <td>[hey, guy, Im, tom, tech, chap, someth, littl,...</td>\n",
       "      <td>14</td>\n",
       "      <td>0.421258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-g1mHQwkpQY</td>\n",
       "      <td>[hi, guy, welcom, back, tech, chap, peopl, thi...</td>\n",
       "      <td>26</td>\n",
       "      <td>0.535237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-lCQMFC2D5Q</td>\n",
       "      <td>[oh, that, way, heavier, I, expect, actual, Im...</td>\n",
       "      <td>26</td>\n",
       "      <td>0.594496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-My0ls6Da-c</td>\n",
       "      <td>[hey, guy, im, tummi, tech, chapman, ive, get,...</td>\n",
       "      <td>26</td>\n",
       "      <td>0.454431</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          file                                              stems  topicID  \\\n",
       "0  -7gyHZEving  [hey, guy, anton, tech, chap, new, msi, GF, 65...       26   \n",
       "1  -cOYX11AfPc  [hey, guy, Im, tom, tech, chap, someth, littl,...       14   \n",
       "2  -g1mHQwkpQY  [hi, guy, welcom, back, tech, chap, peopl, thi...       26   \n",
       "3  -lCQMFC2D5Q  [oh, that, way, heavier, I, expect, actual, Im...       26   \n",
       "4  -My0ls6Da-c  [hey, guy, im, tummi, tech, chapman, ive, get,...       26   \n",
       "\n",
       "   confidence  \n",
       "0    0.535545  \n",
       "1    0.421258  \n",
       "2    0.535237  \n",
       "3    0.594496  \n",
       "4    0.454431  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"topic\"] = df['stems'].apply(lambda processedDoc:\n",
    "    sorted(lda_model_tfidf[dictionary.doc2bow(processedDoc)], key=lambda tup: -1*tup[1])[0]\n",
    ")\n",
    "df[\"topicID\"] = df[\"topic\"].apply(lambda x: x[0])\n",
    "df[\"confidence\"] = df[\"topic\"].apply(lambda x: x[1])\n",
    "df = df.drop([\"topic\"],1)\n",
    "df[\"file\"] = df[\"file\"].apply(lambda x: x[:-4]) \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename({\"file\":\"videoId\"}, axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting the dataframe into a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"videosAndTopics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
